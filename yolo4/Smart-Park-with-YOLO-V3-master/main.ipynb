{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhigh\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "import torch\n",
    "import glob\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Cython\n",
      "  Downloading Cython-0.29.32-py2.py3-none-any.whl (986 kB)\n",
      "     -------------------------------------- 986.3/986.3 kB 9.0 MB/s eta 0:00:00\n",
      "Installing collected packages: Cython\n",
      "Successfully installed Cython-0.29.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'coco'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\mhigh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (58.1.0)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-65.3.0-py3-none-any.whl (1.2 MB)\n",
      "     ---------------------------------------- 1.2/1.2 MB 11.1 MB/s eta 0:00:00\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 58.1.0\n",
      "    Uninstalling setuptools-58.1.0:\n",
      "      Successfully uninstalled setuptools-58.1.0\n",
      "Successfully installed setuptools-65.3.0\n",
      "Requirement already satisfied: wheel in c:\\users\\mhigh\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.37.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install Cython\n",
    "!git clone https://github.com/waleedka/coco\n",
    "!pip install -U setuptools\n",
    "!pip install -U wheel\n",
    "\n",
    "os.getcwd()\n",
    "!mkdir ak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mhigh\\AppData\\Roaming\\Python\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import time\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import cv2 \n",
    "from util import *\n",
    "from darknet import Darknet\n",
    "from preprocess import prep_image, inp_to_image, letterbox_image\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle as pkl\n",
    "import argparse\n",
    "\n",
    "\n",
    "def get_test_input(input_dim, CUDA):\n",
    "    img = cv2.imread(\"Khare_frame_02.png\")\n",
    "    img = cv2.resize(img, (input_dim, input_dim)) \n",
    "    img_ =  img[:,:,::-1].transpose((2,0,1))\n",
    "    img_ = img_[np.newaxis,:,:,:]/255.0\n",
    "    img_ = torch.from_numpy(img_).float()\n",
    "    img_ = Variable(img_)\n",
    "    \n",
    "    if CUDA:\n",
    "        img_ = img_.cuda()\n",
    "    \n",
    "    return img_\n",
    "\n",
    "def prep_image(img, inp_dim):\n",
    "    \"\"\"\n",
    "    Prepare image for inputting to the neural network. \n",
    "    \n",
    "    Returns a Variable \n",
    "    \"\"\"\n",
    "\n",
    "    orig_im = img\n",
    "    dim = orig_im.shape[1], orig_im.shape[0]\n",
    "    img = (letterbox_image(orig_im, (inp_dim, inp_dim)))\n",
    "    img_ = img[:,:,::-1].transpose((2,0,1)).copy()\n",
    "    img_ = torch.from_numpy(img_).float().div(255.0).unsqueeze(0)\n",
    "    return img_, orig_im, dim\n",
    "\n",
    "def write(x, img, classes):\n",
    "    c1 = tuple(x[1:3].int())\n",
    "    c2 = tuple(x[3:5].int())\n",
    "    cls = int(x[-1])\n",
    "    label = \"{0}\".format(classes[cls])\n",
    "    color = (0,0,255)\n",
    "    cv2.rectangle(img, c1, c2,color, 1)\n",
    "    t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN, 1 , 1)[0]\n",
    "    c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "    cv2.rectangle(img, c1, c2,color, -1)\n",
    "    cv2.putText(img, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN, 1, [225,255,255], 1);\n",
    "    return img\n",
    "\n",
    "def arg_parse():\n",
    "    \"\"\"\n",
    "    Parse arguements to the detect module\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='YOLO v3 Video Detection Module')\n",
    "   \n",
    "    parser.add_argument(\"--video\", dest = 'video', help = \n",
    "                        \"Video to run detection upon\",\n",
    "                        default = \"ff.mp4\", type = str)\n",
    "    parser.add_argument(\"--dataset\", dest = \"dataset\", help = \"Dataset on which the network has been trained\", default = \"pascal\")\n",
    "    parser.add_argument(\"--confidence\", dest = \"confidence\", help = \"Object Confidence to filter predictions\", default = 0.5)\n",
    "    parser.add_argument(\"--nms_thresh\", dest = \"nms_thresh\", help = \"NMS Threshhold\", default = 0.4)\n",
    "    parser.add_argument(\"--cfg\", dest = 'cfgfile', help = \n",
    "                        \"Config file\",\n",
    "                        default = \"./cfg/yolov3.cfg\", type = str)\n",
    "    parser.add_argument(\"--weights\", dest = 'weightsfile', help = \n",
    "                        \"weightsfile\",\n",
    "                        default = \"yolov3.weights\", type = str)\n",
    "    parser.add_argument(\"--reso\", dest = 'reso', help = \n",
    "                        \"Input resolution of the network. Increase to increase accuracy. Decrease to increase speed\",\n",
    "                        default = \"704\", type = str)\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_videoprocess():\n",
    "    batch_size = 15\n",
    "    args = arg_parse()\n",
    "    confidence = float(args.confidence)\n",
    "    nms_thesh = float(args.nms_thresh)\n",
    "    start = 0\n",
    "\n",
    "    CUDA = torch.cuda.is_available()\n",
    "\n",
    "    num_classes = 1\n",
    "\n",
    "    CUDA = torch.cuda.is_available()\n",
    "    \n",
    "    bbox_attrs = 5 + num_classes\n",
    "    \n",
    "    print(\"Loading network.....\")\n",
    "    model = Darknet(args.cfgfile)\n",
    "    model.load_weights(args.weightsfile)\n",
    "    print(\"Network successfully loaded\")\n",
    "\n",
    "    model.net_info[\"height\"] = args.reso\n",
    "    inp_dim = int(model.net_info[\"height\"])\n",
    "    #uncomment this if error occurs due to frame size\n",
    "#     assert inp_dim % 32 == 0 \n",
    "#     assert inp_dim > 32\n",
    "\n",
    "    if CUDA:\n",
    "        model.cuda()\n",
    "        \n",
    "    model(get_test_input(inp_dim, CUDA), CUDA)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    videofile = args.video\n",
    "    \n",
    "    cap = cv2.VideoCapture(videofile)\n",
    "    \n",
    "    assert cap.isOpened(), 'Cannot capture source'\n",
    "    frame_count = 0\n",
    "    frames = []\n",
    "    start = time.time()    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frame_count += 1\n",
    "            frames.append(frame)\n",
    "#             print(\"appended\")\n",
    "            if len(frames) == batch_size:\n",
    "              for i, item in enumerate(frames):\n",
    "                frame = item\n",
    "                img, orig_im, dim = prep_image(frame, inp_dim)\n",
    "\n",
    "                im_dim = torch.FloatTensor(dim).repeat(1,2)                        \n",
    "\n",
    "\n",
    "                if CUDA:\n",
    "                    im_dim = im_dim.cuda()\n",
    "                    img = img.cuda()\n",
    "\n",
    "                with torch.no_grad():   \n",
    "                    output = model(Variable(img), CUDA)\n",
    "                output = write_results(output, confidence, num_classes, nms = True, nms_conf = nms_thesh)\n",
    "                print(\"count cars {}\".format(output.size(0)))\n",
    "\n",
    "                if type(output) == int:\n",
    "                    frame_count+=1\n",
    "                    print(\"FPS of the video is {:5.2f}\".format( frame_count / (time.time() - start)))\n",
    "                    cv2.imshow(\"frame\", orig_im)\n",
    "                    key = cv2.waitKey(1)\n",
    "                    if key & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    continue\n",
    "\n",
    "\n",
    "                im_dim = im_dim.repeat(output.size(0), 1)\n",
    "                scaling_factor = torch.min(inp_dim/im_dim,1)[0].view(-1,1)\n",
    "\n",
    "                output[:,[1,3]] -= (inp_dim - scaling_factor*im_dim[:,0].view(-1,1))/2\n",
    "                output[:,[2,4]] -= (inp_dim - scaling_factor*im_dim[:,1].view(-1,1))/2\n",
    "\n",
    "                output[:,1:5] /= scaling_factor\n",
    "\n",
    "                for i in range(output.shape[0]):\n",
    "                    output[i, [1,3]] = torch.clamp(output[i, [1,3]], 0.0, im_dim[i,0])\n",
    "                    output[i, [2,4]] = torch.clamp(output[i, [2,4]], 0.0, im_dim[i,1])\n",
    "\n",
    "                classes = load_classes('data/coco.names')\n",
    "                colors = pkl.load(open(\"pallete\", \"rb\"))\n",
    "\n",
    "                list(map(lambda x: write(x, orig_im, classes), output))\n",
    "                empty = 6 - output.size(0)\n",
    "                cv2.putText(orig_im, \"Total empty spots: \" + str(empty), (5,30), cv2.FONT_HERSHEY_SIMPLEX, 2, [255,255,255], 9, cv2.LINE_AA)\n",
    "\n",
    "    #             cv2.imshow(\"Parking 1\", orig_im)\n",
    "    #             frames += 1\n",
    "                name = '{0}.jpg'.format(frame_count + i - batch_size)\n",
    "\n",
    "                name = os.path.join('./ak', name)\n",
    "                cv2.imwrite(name, frame)\n",
    "\n",
    "                \n",
    "                key = cv2.waitKey(1)\n",
    "                if key & 0xFF == ord('q'):\n",
    "                    break\n",
    "              print(\"FPS of the video is {:5.2f}\".format( frame_count / (time.time() - start)))\n",
    "              \n",
    "              frames = []\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    images = list(glob.iglob(os.path.join('./ak', '*.*')))\n",
    "    images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
    "    \n",
    "    make_video('./out.mp4', images, fps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames per second using video.get(cv2.CAP_PROP_FPS) : 0.0\n"
     ]
    }
   ],
   "source": [
    "args = arg_parse()\n",
    "videofile = args.video\n",
    "video = cv2.VideoCapture(videofile)\n",
    "\n",
    "# Find OpenCV version\n",
    "(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')\n",
    "\n",
    "if int(major_ver)  < 3 :\n",
    "    fps = video.get(cv2.cv.CV_CAP_PROP_FPS)\n",
    "    print(\"Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}\".format(fps))\n",
    "else :\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    print(\"Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}\".format(fps))\n",
    "\n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all image file paths to a list.\n",
    "# Sort the images by name index.\n",
    "# images = sorted(images, key=lambda x: float(os.path.split(x)[1][:-3]))\n",
    "\n",
    "def make_video(outvid, images=None, fps=25, size=None,\n",
    "               is_color=True, format=\"FMP4\"):\n",
    "    \"\"\"\n",
    "    Create a video from a list of images.\n",
    " \n",
    "    @param      outvid      output video\n",
    "    @param      images      list of images to use in the video\n",
    "    @param      fps         frame per second\n",
    "    @param      size        size of each frame\n",
    "    @param      is_color    color\n",
    "    @param      format      see http://www.fourcc.org/codecs.php\n",
    "    @return                 see http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_gui/py_video_display/py_video_display.html\n",
    "    \"\"\"\n",
    "    from cv2 import VideoWriter, VideoWriter_fourcc, imread, resize\n",
    "    fourcc = VideoWriter_fourcc(*format)\n",
    "    vid = None\n",
    "    for image in images:\n",
    "        if not os.path.exists(image):\n",
    "            raise FileNotFoundError(image)\n",
    "        img = imread(image)\n",
    "        if vid is None:\n",
    "            if size is None:\n",
    "                size = img.shape[1], img.shape[0]\n",
    "            vid = VideoWriter(outvid, fourcc, float(fps), size, is_color)\n",
    "        if size[0] != img.shape[1] and size[1] != img.shape[0]:\n",
    "            img = resize(img, size)\n",
    "        vid.write(img)\n",
    "    vid.release()\n",
    "    return vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading network.....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mhigh\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\conv.py:131: UserWarning: Failed to initialize NumPy: module compiled against API version 0x10 but this version of numpy is 0xf (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:68.)\n",
      "  self.weight = Parameter(torch.empty(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'yolov3.weights'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43myolo_videoprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [2], line 18\u001b[0m, in \u001b[0;36myolo_videoprocess\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading network.....\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m Darknet(args\u001b[38;5;241m.\u001b[39mcfgfile)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweightsfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNetwork successfully loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mnet_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mreso\n",
      "File \u001b[1;32mc:\\CS\\capstone\\yolo4\\Smart-Park-with-YOLO-V3-master\\darknet.py:388\u001b[0m, in \u001b[0;36mDarknet.load_weights\u001b[1;34m(self, weightfile)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_weights\u001b[39m(\u001b[39mself\u001b[39m, weightfile):\n\u001b[0;32m    386\u001b[0m     \n\u001b[0;32m    387\u001b[0m     \u001b[39m#Open the weights file\u001b[39;00m\n\u001b[1;32m--> 388\u001b[0m     fp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(weightfile, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    390\u001b[0m     \u001b[39m#The first 4 values are header information \u001b[39;00m\n\u001b[0;32m    391\u001b[0m     \u001b[39m# 1. Major version number\u001b[39;00m\n\u001b[0;32m    392\u001b[0m     \u001b[39m# 2. Minor Version Number\u001b[39;00m\n\u001b[0;32m    393\u001b[0m     \u001b[39m# 3. Subversion number \u001b[39;00m\n\u001b[0;32m    394\u001b[0m     \u001b[39m# 4. IMages seen \u001b[39;00m\n\u001b[0;32m    395\u001b[0m     header \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfromfile(fp, dtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mint32, count \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov3.weights'"
     ]
    }
   ],
   "source": [
    "yolo_videoprocess() #main program for detecting number of cars in the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3c926f88ea83e26a1306b115f53557d080c1cbceabc51284dddb917b263a059"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
